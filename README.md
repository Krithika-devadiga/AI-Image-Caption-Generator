# ğŸ§  AI Image Captioning Web App

This project is a Flask-based web application that uses a Transformer-based deep learning model to generate **automatic captions for images**. Upload any image, and get a meaningful caption generated by 

## âœ¨ Features

- ğŸ“· Upload image through a simple UI
- ğŸ§  Generates intelligent image captions using Transformers
- ğŸ”Š Uses pre-trained models from Hugging Face (`blip-image-captioning-base`)
- ğŸŒ Web interface built with HTML, CSS, and Flask
- ğŸ“¦ Lightweight and easy to deploy

---

## ğŸš€ Technologies Used

| Tool        | Purpose                                  |
|-------------|------------------------------------------|
| **Flask**   | Web backend framework (Python)           |
| **Torch**   | PyTorch for model inference              |
| **Transformers** | Pretrained Hugging Face captioning model |
| **Pillow**  | Image processing in Python               |
| **HTML/CSS**| Web front-end                            |

---


## ğŸ“ Project Structure

- `flask-captioner/`: Contains the Flask backend files including the deep learning model, captioning logic, and the main app file (app.py).

- `index.html`: The main HTML frontend for uploading images and displaying AI-generated captions.

- `style.css`: CSS file used for UI design and animated background effects to enhance user experience.

- `script.js`: (Optional) JavaScript file for adding interactivity to the frontend.

- `.gitignore`: Specifies files and folders (like __pycache__ or large model files) to be excluded from version control.

- `README.md`: This documentation file that describes the projectâ€™s purpose, setup steps, and usage instructions.

---

## ğŸ–¼ï¸ How It Works

1. User uploads an image through the form.
2. Flask handles the image backend and loads the model.
3. Model processes the image and generates a caption.
4. Caption is displayed on the web page below the image.

---

## ğŸ› ï¸ Setup Instructions

1. **Clone this repository**
 
git clone https://github.com/yourusername/ai-image-captioning.git
cd ai-image-captioning


2. **Create and activate virtual environment**
python -m venv venv
source venv/bin/activate  # for Linux/Mac
venv\Scripts\activate     # for Windows

3. **Install dependencies**
pip install flask torch transformers pillow

4. **Run the app**
python app.py

5. **Visit in browser**
http://127.0.0.1:5000

---

## ğŸ§  Model Used
* BLIP Image Captioning Model
* Pretrained on a large dataset of image-text pairs
* Available via Hugging Face transformers library